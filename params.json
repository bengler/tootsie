{"name":"Tootsie","body":"Tootsie is a simple, robust, scalable audio/video/image transcoding/modification application. Tootsie can transcode audio, video and images between different formats, and also perform basic manipulations such as photo scaling and cropping, for generating thumbnails and photos at different resolutions.\r\n\r\nTootsie is divided into multiple independent parts:\r\n\r\n* Job manager: finds new transcoding jobs and executes them.\r\n* FFmpeg, ImageMagick: performs the actual transcoding.\r\n* Queue: stores pending tasks.\r\n* Storage: loads and saves the file data.\r\n* Web service: A small RESTful API for managing jobs.\r\n\r\nFor storage, Tootsie supports web resources (ie., reading with HTTP GET, writing with HTTP POST) and Amazon S3 buckets.\r\n\r\nFor queues, Tootsie supports local file-based queues (mostly suitable for testing), AMQP queues (eg., RabbitMQ) and Amazon Simple Queue Service (aka SQS).\r\n\r\nThe framework is designed to be easily pluggable, and to let you pick the parts you need to build a custom transcoding service. It is also designed to be easily distributed across many nodes.\r\n\r\nJobs\r\n----\r\n\r\nThe task manager pops jobs from a queue and processes them. Each job specifies an input, an output, and transcoding parameters. Optionally the job may also specify a notification URL which is invoked to inform the caller about job progress.\r\n\r\nSupported inputs:\r\n\r\n* HTTP resource. Currently only public (non-authenticated) resources are supported.\r\n* Amazon S3 bucket resource. S3 buckets must have the appropriate ACLs so that Tootsie can read the files; if the input file is not public, Tootsie must be run with an AWS access key that is granted read access to the file.\r\n\r\nSupported outputs:\r\n\r\n* HTTP resource. The encoded file will be `POST`ed to a URL.\r\n* Amazon S3 bucket resource. Tootsie will need write permissions to any S3 buckets.\r\n\r\nEach job may have multiple outputs given a single input. Designwise, the reason for doing this -- as opposed to requiring that the client submit multiple jobs, one for each output -- is twofold:\r\n\r\n1. It allows the job to cache the input data locally for the duration of the job, rather than fetching it multiple times. One could suppose that multiple jobs could share the same cached input, but this would be awkward in a distributed setting where each node has its own file system; in such a case, a shared storage mechanism (file system, database or similar) would be needed.\r\n\r\n2. It allows the client to be informed when *all* transcoded versions are available, something which may drastically simplify client logic. For example, a web application submitting a job to produce multiple scaled versions of an image may only start showing these images when all versions have been produced. To know whether all versions have been produced, it needs to maintain state somewhere about the progress. Having a single job produce all versions means this state can be reduced to a single boolean value.\r\n\r\nWhen using multiple outputs per job one should keep in mind that this reduces job throughput, requiring more concurrent job workers to be deployed.\r\n\r\nFFmpeg and ImageMagick are invoked for each job to perform the transcoding. These are abstracted behind set of generic options specifying format, codecs, bit rate and so on.\r\n\r\nAPI\r\n===\r\n\r\nTo schedule jobs, use the REST service:\r\n\r\n* POST `/job`: Schedule a job. Returns 201 if the job was created.\r\n* GET `/status`: Get current processing status as a JSON hash.\r\n\r\nThe job must be posted as an JSON hash with the content type `application/json`. Common to all job scheduling POSTs are these keys:\r\n\r\n* `type`: Type of job. See sections below for details.\r\n* `notification_url`: Optional notification URL. Progress (including completion and failure) will be reported using POSTs.\r\n* `retries`: Maximum number of retries, if any. Defaults to 5.\r\n\r\nJob-specific parameters are provided in the key `params`.\r\n\r\nVideo transcoding jobs\r\n----------------------\r\n\r\nVideo jobs have the `type` key set to either `video`, `audio`. Currently, `audio` is simply an alias for `video` and handled by the same pipeline. The key `params` must be set to a hash with these keys:\r\n\r\n* `input_url`: URL to input file, either an HTTP URL or an S3 URL (see below).\r\n* `versions`: Either a hash or an array of such hashes, each with the following keys:\r\n  * `target_url`: URL to output resource, either an HTTP URL which accepts POSTs, or an S3 URL.\r\n  * `thumbnail`: If specified, a thumbnail will be generated based on the options in this hash with the following keys:\r\n    * `target_url`: URL to output resource, either an HTTP URL which accepts POSTs, or an S3 URL.\r\n    * `width`: Desired width of thumbnail, defaults to output width.\r\n    * `height`: Desired height of thumbnail, defaults to output height.\r\n    * `at_seconds`: Desired point (in seconds) at which the thumbnail frame should be captured. Defaults to 50% into stream.\r\n    * `at_fraction`: Desired point (in percentage) at which the thumbnail frame should be captured. Defaults to 50% into stream.\r\n    * `force_aspect_ratio`: If `true`, force aspect ratio; otherwise aspect is preserved when computing dimensions.\r\n  * `audio_sample_rate`: Audio sample rate, in hertz.\r\n  * `audio_bitrate`: Audio bitrate, in bits per second.\r\n  * `audio_codec`: Audio codec name, eg. `mp4`.\r\n  * `video_frame_rate`: video frame rate, in hertz.\r\n  * `video_bitrate`: video bitrate, in bits per second.\r\n  * `video_codec`: video codec name, eg. `mp4`.\r\n  * `width`: desired video frame width in pixels.\r\n  * `height`: desired video frame height in pixels.\r\n  * `format`: File format.\r\n  * `content_type`: Content type of resultant file. Tootsie will not be able to guess this at the moment.\r\n\r\nCompletion notification provides the following data:\r\n\r\n* `outputs` contains an array of results. Each is a hash with the following keys:\r\n  * `url`: the completed file.\r\n  * `metadata`: image metadata as a hash. These are raw EXIF and IPTC data from ImageMagick.\r\n\r\nImage transcoding jobs\r\n----------------------\r\n\r\nImage jobs have the `type` key set to `image`. The key `params` must be set to a hash with these keys:\r\n\r\n* `input_url`: URL to input file, either an HTTP URL, `file:/path` URL or an S3 URL (see below).\r\n* `versions`: Either a hash or an array of such hashes, each with the following keys:\r\n  * `target_url`: URL to output resource, either an HTTP URL, `file:/path` URL which accepts POSTs, or an S3 URL.\r\n  * `width`: Optional desired width of output image.\r\n  * `height`: Optional desired height of output image.\r\n  * `scale`: One of the following values:\r\n    * `down` (default): The input image is scaled to fit within the dimensions `width` x `height`. If only `width` or only `height` is specified, then the other component will be computed from the aspect ratio of the input image.\r\n    * `up`: As `within`, but allow scaling to dimensions that are larger than the input image.\r\n    * `fit`: Similar to `down`, but the dimensions are chosen so the output width and height are always met or exceeded. In other words, if you pass in an image that is 100x50, specifying output dimensions as 100x100, then the output image will be 150x100.\r\n    * `none`: Don't scale at all.\r\n  * `crop`: If true, crop the image to the output dimensions.\r\n  * `format`: Either `jpeg`, `png` or `gif`.\r\n  * `quality`: A quality value between 0.0 and 1.0 which will be translated to a compression level depending on the output coding. The default is 1.0.\r\n  * `strip_metadata`: If true, metadata such as EXIF and IPTC will be deleted. For thumbnails, this often reduces the file size considerably.\r\n  * `medium`: If `web`, the image will be optimized for web usage. See below for details.\r\n  * `content_type`: Content type of resultant file. The system will be able to guess basic types such as `image/jpeg`.\r\n\r\nNote that scaling always preserves the aspect ratio of the original image; in other words, if the original is 100 x 200, then passing the dimensions 100x100 will produce an image that is 50x100. Enabling cropping, however, will force the aspect ratio of the specified dimensions.\r\n\r\nIf the option `medium` specifies `web`, the following additional transformations will be performed:\r\n\r\n* The image will be automatically rotated based on EXIF orientation metadata, since web browsers don't do this.\r\n* CMYK images will be converted to RGB, since most web browsers don't seem to display CMYK correctly.\r\n\r\nCompletion notification provides the following data:\r\n\r\n* `outputs` contains an array of results. Each is a hash with the following keys:\r\n  * `url`: URL for the completed file.\r\n* `metadata`: image metadata as a hash. These are raw EXIF and IPTC data from ImageMagick.\r\n* `width`: width, in pixels, of original image.\r\n* `height`: height, in pixels, of original image.\r\n* `depth`: depth, in bits, of original image.\r\n\r\nNotifications\r\n-------------\r\n\r\nIf a notification URL is provided, events will be sent to it using `POST` requests as JSON data. These are 'fire and forget' and will currently not be retried on failure, and the response status code is ignored.\r\n\r\nThere are several types of events, indicated by the `event` key:\r\n\r\n* `started`: The job was started.\r\n* `complete`: The job was complete. The key `time_taken` will contain the time taken for the job, in seconds. Additional data will be provided that are specific to the type of job.\r\n* `failed`: The job failed. The key `reason` will contain a textual explanation for the failure.\r\n* `failed_will_retry`: The job failed, but is being rescheduled for retrying. The key `reason` will contain a textual explanation for the failure.\r\n\r\nResource URLs\r\n-------------\r\n\r\nTootsie supports referring to inputs and outputs using URLs, namely `file:` and `http:`. Additionally, Tootsie supports its own proprietary S3 URL format.\r\n\r\nTo specify S3 URLs, we use a custom URI format:\r\n\r\n    s3:<bucketname></path/to/file>[?<options>]\r\n\r\nThe components are:\r\n\r\n* bucketname: The name of the S3 bucket.\r\n* /path/to/file: The actual S3 key.\r\n* options: Optional parameters for storage, an URL query string.\r\n\r\nThe options are:\r\n\r\n* `acl`: One of `private` (default), `public-read`, `public-read-write` or `authenticated-read`.\r\n* `storage_class`: Either `standard` (default) or `reduced_redundancy`.\r\n* `content_type`: Override stored content type.\r\n\r\nExample S3 URLs:\r\n\r\n* `s3:myapp/video`\r\n* `s3:myapp/thumbnails?acl=public-read&storage_class=reduced_redundancy`\r\n* `s3:myapp/images/12345?content_type=image/jpeg`\r\n\r\nRequirements\r\n============\r\n\r\n* Ruby 1.8.7 or later.\r\n\r\nFor video jobs:\r\n\r\n* FFmpeg\r\n\r\nFor image jobs:\r\n\r\n* ImageMagick\r\n* Exiv2\r\n* pngcrush (optional)\r\n\r\nOptional dependencies:\r\n\r\n* Amazon S3 for loading and storage of files.\r\n* AMQP-compliant server (such as RabbitMQ) or Amazon Simple Queue Service for internal task queue management.\r\n\r\nInstallation\r\n============\r\n\r\n`gem install tootsie`\r\n\r\nRunning\r\n=======\r\n\r\nCreate a configuration, eg. `tootsie.conf`:\r\n\r\n    --- \r\n      queue:\r\n        adapter: sqs\r\n        queue: tootise\r\n      aws_access_key_id: <your Amazon key>\r\n      aws_secret_access_key: <your Amazon secret>\r\n      pid_path: <where to write pid file>\r\n      log_path: <where to write log file>\r\n      worker_count: <number of workers>\r\n\r\nStart the task manager with `tootsie -c tootsie.conf`.\r\n\r\nNow create a rackup file, and call it `config.ru`:\r\n\r\n    require 'tootsie'\r\n    Tootsie::Application.new.configure!('tootsie.conf')\r\n    run Tootsie::WebService\r\n\r\nTo run the web service, you will need a Rack-compatible web server, such as Unicorn or Thin. To start with Thin on port 9090:\r\n\r\n    $ thin --daemonize --rackup config.ru --port 9090 start\r\n\r\nJobs may now be posted to the web service API. For example:\r\n\r\n    $ cat << END | curl -d @- http://localhost:9090/job\r\n    {\r\n      \"type\": \"video\",\r\n      \"notification_url\": \"http://example.com/transcoder_notification\",\r\n      \"params\": {\r\n        \"input_url\": \"http://example.com/test.3gp\",\r\n        \"versions\": {\r\n          \"target_url\": \"s3:mybucket/test.mp4?acl=public_read\",\r\n          \"audio_sample_rate\": 44100,\r\n          \"audio_bitrate\": 64000,\r\n          \"format\": \"flv\",\r\n          \"content_type\": \"video/x-flv\"\r\n        }\r\n      }\r\n    }\r\n    END\r\n\r\nConfiguration\r\n=============\r\n\r\nThe configuration is a YAML document with the following keys:\r\n\r\n    aws_access_key_id: <your Amazon key>\r\n    aws_secret_access_key: <your Amazon secret>\r\n    pid_path: <where to write pid file>\r\n    log_path: <where to write log file>\r\n    worker_count: <number of workers>\r\n    queue:\r\n      <... queue options ...>\r\n\r\nThe queue options is a hash with a key `adapter` telling Tootsie which queue implementation to use.\r\n\r\nThe `sqs` adapter takes the following options:\r\n\r\n    queue_name: <name of queue, defaults to 'tootsie'>\r\n    max_backoff: <max seconds to wait when queue is empty, defaults to 2>\r\n\r\nFor the `amqp` adapter:\r\n\r\n    queue_name: <name of queue, defaults to 'tootsie'>\r\n    host_name: <host name of AMQP server, defaults to localhost>\r\n    max_backoff: <max seconds to wait when queue is empty, defaults to 2>\r\n\r\nFor the `file` adapter:\r\n\r\n    root: <directory to store files>\r\n\r\nNote that when running a large number of workers, you should increase the backoff interval to avoid incurring a lot of queue requests.","tagline":"Tootsie is an audio and video transcoding that suppots Amazon S3 and Amazon SQS. MIT license.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}